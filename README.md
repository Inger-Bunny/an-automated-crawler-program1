# 系统设计文档<br>
## 1. 系统概述<br>  
  本系统是基于Python编写的自动化爬虫程序，专门用于从链家网上海二手房房源页面高效地爬取数据。通过使用requests库发送HTTP请求，系统能够获取网页内容，并利用parsel库对HTML进行解析，从中提取房源的标题、地区、户型、面积等详细信息。这些数据经过严格的数据过滤和去重处理后，被存储到一个结构化的SQLite数据库中，确保了数据的持久化存储和管理。  
## 2. 系统架构  
  系统主要由以下几个组成部分构成：  
  爬虫模块 (`scrape.py`)：负责发送HTTP请求、解析HTML内容，并提取房源信息。  
  数据存储模块 (`database.py`)：定义了SQLite数据库的表结构，并提供了插入数据和加载数据的功能。  
  日志模块 (`logging`)：使用Python内置的logging模块记录程序运行过程中的关键信息和错误。  
  主程序 (`main.py`)：整合了爬虫模块和数据存储模块，实现了完整的爬虫流程和数据存储功能。  
## 3. 系统流程  
### 3.1.爬虫流程
   程序会根据给定的页面编号构造链家二手房的页面URL，并使用requests库发送HTTP请求来获取页面内容。随后，使用parsel库解析HTML内容，从页面中提取房源的标题、地区、户型、面积等详细信息。在存储数据前，会进行数据过滤，以避免重复存储已存在的数据。最终，有效的房源数据将被存储到一个SQLite数据库中，以确保数据的持久性和管理。  
### 3.2.数据存储流程  
    首先创建一个SQLite数据库表，定义了包含房源详细信息的表结构，以确保数据存储的一致性和完整性。随后，通过爬虫模块获取的房源信息将被插入到数据库中，确保数据的持久化存储。在爬取和存储过程中，程序会加载数据库中已存在的房源详情页URL，用于去重和监控，确保不重复获取已经抓取过的页面内容。这种整合方式能够有效管理和利用房源数据，使得爬取和存储操作更为高效和可靠。  
### 3.3. 系统设计考虑因素  
    这段程序设计具备良好的可扩展性，允许在需要时轻松添加新的功能模块或调整现有功能。例如，可以通过添加新的数据提取规则或引入新的数据处理流程来扩展功能。通过模块化和清晰的函数设计，不同的功能模块可以独立开发和测试，便于整合到现有系统中。
为确保稳定性，程序使用了异常处理机制来捕获并处理可能的网络波动或数据解析错误。通过适当的日志记录，系统能够记录关键事件和异常情况，为故障排查提供足够的信息和上下文。<br>这种做法不仅提升了系统的可靠性，还能加快问题定位和解决的速度。  
    在数据管理方面，数据库表的设计和数据存取函数的设计严格遵循一致性和准确性的原则。定义清晰的数据结构和有效的数据校验机制，确保存储到数据库中的房源信息是完整和正确的。此外，通过事务管理和适当的数据验证，进一步加强了数据一致性，防止无效或不完整的数据影响系统的正常运行。
综上所述，这种综合设计不仅能够保证系统在面对各种挑战时保持稳定，还能够有效管理和维护数据的一致性和完整性，确保系统长期稳定运行并满足业务需求的变化和扩展。  
### 3.4. 技术选择  
Python：作为主要开发语言，用于编写爬虫逻辑和数据处理逻辑。  
requests：用于发送HTTP请求获取网页内容。  
parsel：用于解析HTML内容，提取所需信息。  
SQLite：作为本地数据库，用于存储爬取的房源数据。  
logging：Python标准库，用于记录程序运行过程中的日志信息。  


# 说明文档 
## 文件结构<br>
scraper.py：主爬虫脚本文件，包含数据提取、爬取和存储的所有功能。<br>
house_data.db：SQLite数据库文件，用于存储爬取的房源数据。<br>
## 功能描述<br>
提取房源信息：<br>
从页面中的每个房源项中提取房源标题、链接、区域、户型、面积、朝向、装修情况、楼层、建造时间、房子类型、标签、总价、单价、关注人数和发布时间等信息。<br>
爬取页面数据：<br>
爬取指定页面的数据，并提取房源信息。支持分页爬取。<br>
创建数据库表：<br>
如果数据库中不存在表格，则自动创建一个用于存储房源数据的表格。<br>
插入数据到数据库：<br>
将提取的房源数据插入到SQLite数据库中，避免重复插入。<br>
加载已存在的数据：<br>
从数据库中加载已存在的数据链接，避免重复爬取相同的房源信息。<br>
主函数：<br>
执行爬虫任务，包括创建数据库表、爬取页面、插入数据等，并控制爬取频率<br>  

# 使用文档<br>  
## 1.环境要求<br> 
Python 3.x<br> 
安装以下库：<br> 
 requests<br> 
 parsel<br>
 sqlite3（Python内置模块）<br>
## 2.使用步骤<br>
### 2.1. 安装依赖库<br>
   pip install requests parsel<br>
### 2.2.设置数据库<br>
   确保已安装SQLite，并在同一目录下创建名为 `house_data.db` 的SQLite数据库文件。<br>
### 2.3.配置日志<br>
   日志级别默认为INFO级别，日志格式包括时间戳、日志级别和消息内容。<br>
### 2.4.运行主程序<br>
   在命令行或终端中运行 `main.py` 文件：<br>  
   ![image](https://github.com/user-attachments/assets/1304041c-00e9-48d9-ada8-a8130f8c14e6)<br>

   程序将开始爬取链家网上海二手房的房源数据。每爬取一页数据后，会将新数据写入数据库，并在日志中记录成功写入的房源信息。<br>
### 2.5.数据存储和管理<br>
   数据库表结构已事先定义，包括房源的详细信息字段。<br>
   程序会在每次运行时检查数据库中已存在的房源详情页URL，确保不重复存储已抓取过的数据。<br>
### 2.6.定时任务和自动化<br>
   如需定期更新数据，可以设置操作系统的定时任务或使用Python的定时任务模块（如APScheduler），定期执行 `main.py`。<br>
## 3.注意事项<br>
爬取过程中可能会受到链家网站的访问限制或反爬虫机制影响，建议适当控制爬取频率以避免被封IP。<br>
确保合法使用爬虫程序，并遵守链家网站的使用条款和Robots协议。<br>
## 4.扩展和定制<br>
可根据需求扩展系统功能，如数据分析和可视化模块，定制化数据存储和处理流程，或支持其他城市的二手房数据爬取。<br>
## 5.示例<br>
以下是一个示例配置，展示如何设置数据库和运行主程序： <br>
![image](https://github.com/user-attachments/assets/19cb6ac0-e744-4a35-acb1-cf134cd7b702)<br>

# 测试文档<br>  
## 1. 目标<br>
测试爬虫脚本的功能是否按预期工作，包括数据提取、存储到数据库、以及处理不同页面的情况。<br>

## 2. 环境<br>
Python版本: 3.7+<br>
依赖库: requests, parsel, sqlite3, logging<br>
测试网站: https://sh.lianjia.com/ershoufang/<br>
数据库: SQLite (house_data.db)<br>
## 3. 测试用例<br>
### 3.1: 数据提取<br>
目标: 验证extract_house_info函数是否能正确提取房源信息。<br>
步骤:<br>
从一个房源列表项中提取信息。<br>
检查返回的数据字典中的各字段是否与期望一致。<br>
预期结果:<br>

{<br>
     "润江花苑 3室1厅 南 北",
    "润江花苑 _泗泾",
     "3室1厅",
     " 90.11平米 ",
    " 南 北 ",
    "简装",
   " 中楼层(共25层) ",
     " 2011年 ",
    " 塔楼",
     "VR看装修_房本满五年",
     "255万",
     "28299元/平",
     "41人关注 ",
     " 1个月以前发布",
    "https://sh.lianjia.com/ershoufang/107109922689.html"
}<br>
实际结果:<br>
![image](https://github.com/user-attachments/assets/3f8e608d-659b-430a-a3ff-757a1f595d61)<br>![image](https://github.com/user-attachments/assets/315128dc-ebb5-4b13-8522-bd32427cd81f)<br>
### 3.2: 页面爬取<br>
目标: 验证scrape_page函数是否能成功爬取指定页面的数据。<br>
步骤:<br>
爬取页面 pg16。<br>
确认返回的房源数据符合实际页面内容。<br>
预期结果与提取的数据应该与页面显示的数据一致。<br>
![image](https://github.com/user-attachments/assets/7d4be02d-94cd-41d0-9949-19ee7daf877e)<br>
### 3.3 正确爬取网站1000条以上信息<br>
目标: 验证main函数是否能正确地爬取1000条以上数据，并将数据插入到数据库中。<br>
步骤:<br>
运行main函数。<br>
确认所有页面数据被成功爬取和存储。<br>
预期结果:<br>
所有页面数据应被正确爬取，数据库中应包含完整的数据集。<br>
实际结果:<br>
![image](https://github.com/user-attachments/assets/55d65297-5740-4e43-9413-ce9a1da124fb)<br>





